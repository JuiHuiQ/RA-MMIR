train_params:
  output_dir: output/train
  experiment_name: default
  experiment_tag: default         # name for experiment in wandb logging
  start_epoch: 0                  # use -1 to resume the epoch number from restore_path ckpt given
  batch_size: 2
  num_epochs: 90
  sync_bn: false                  # whether to sync batch norm layer in distributed training setting
  restore_opt: true               # whether to restore the optimizer states while resuming the training from given ckpt
  num_workers: 0                  # number of dataloader workers
  log_interval: 50 
  debug: true                     # if true plots the matched keypoints for one image in every batch during training
  debug_path: "debug"             # folder where debug plots should be stored
  debug_iters: 10                 # number of iters in each epoch for which debug plot should be made
  val_images_count: 10            # should be <= 1500. Number of images to use for computing validation score
  use_wandb: false                # whether to use wandb to log checkpoints to cloud(Useful if training in colab)
  use_ema: false                  # whether to maintain moving average of superglue model weights that will be used in testing and inference
  init_seed: 10

RA_MMIR_params:
  sinkhorn_iterations: 20
  match_threshold: 0.4
  num_layers: 9
  restore_path:                   # path to the ckpt from which training should be resumed
  use_layernorm: false            # whether to use layernorm instead of batchnorm in the MLP layer
  bin_value: 1.0                  # initial value for dustbin in assignment
  pos_loss_weight: 0.45           # weightage for positive(matched points) components of loss
  neg_loss_weight: 1.0            # weightage for negative(non-matched points) component of loss
  tps_target_loss_weight: 0.4

RAMM_Point_params:
  nms_radius: 4
  max_keypoints: 512
  keypoint_threshold: 0.0
  remove_borders: 4
  using_bn: False
  grid_size: 8
  pretrained_model: 'none'        # pretrained_model
  backbone:
    backbone_type: 'VGG'          # backbone
    vgg:
      channels: [ 64,64,64,64,128,128,128,128 ]
  det_head:                      # detector head
    feat_in_dim: 128
  des_head:                      # descriptor head
    feat_in_dim: 128
    feat_out_dim: 256
  det_thresh: 0.001              # 1/65
  nms: 4
  topk: -1

optimizer_params:
  opt_type: adam                  # give either 'adam' or 'sgd'
  lr: 0.0001                      #initial learning rate
  weight_decay: 0.0005            # weight decay to use for weights. Bias components are excluded
  warmup_epochs: 1                # Number of epoch for warming up the learning rate
  step_epoch: 25                  # Epoch number after which learning rate will be exponentially decayed according to step_value below
  step_value: 0.9440608762859234  # Determines the decay rate of learning rate after step_epoch. Curr value is for 40 epochs after which init lr will be divided by 10

dataset_params:
  dataset_path: "L:\\dataset\\COCO\\"                # Path to COCO dataset that contains the 'train2017', 'val2017' and 'annotations' folder
  apply_color_aug: true           # whether to apply photometric distortions
  image_height: 480 
  image_width: 640
  resize_aspect: false            # whether to resize the input image with aspect ratio maintained
  augmentation_params:
    patch_ratio: 0.85 
    # All the below params are ranges for distortion. (0, Mentioned_value).
    # Refer 'get_perspective_mat' function in utils/preprocess_utils.py for more info
    perspective_x: 0.0008         # range for perspective-x component
    perspective_y: 0.0008         # range of perspective-y component
    shear_ratio: 0.04             # shear ratio range
    shear_angle: 10               # shear direction range
    rotation_angle: 25            # rotation angle range
    scale: 0.6                    # uniform scaling range
    translation: 0.6              # translation component range
